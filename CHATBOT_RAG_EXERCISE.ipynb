{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akuzikemoyo/AI-class/blob/main/CHATBOT_RAG_EXERCISE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtwbgMKWKzhL",
        "outputId": "12d178a8-ea35-4d37-cfcc-ab194646cfd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai -q\n",
        "!pip install langchain-community -q\n",
        "!pip install langchain-experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDz7_Zk0PYiZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
        "from typing import TypedDict, List, Optional, Tuple, Dict\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOZUZKOdPYdw"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKWb6OgQPZhZ"
      },
      "outputs": [],
      "source": [
        "class ZhipuAI_embeddings:\n",
        "    def __init__(self, model_name: str = 'embeddings-3'):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"https://open.bigmodel.cn/api/paas/v4\"\n",
        "        self.embedding = self._init_model()\n",
        "    def _init_model(self) -> OpenAIEmbeddings:\n",
        "        return OpenAIEmbeddings(\n",
        "            model=self.model_name,\n",
        "            base_url=self.base_url,\n",
        "            api_key=userdata.get(\"GLYN\")\n",
        "        )\n",
        "\n",
        "embeddings = ZhipuAI_embeddings().embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxC2sEAfPYZq"
      },
      "outputs": [],
      "source": [
        "client = ChatOpenAI(\n",
        "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\",\n",
        "    api_key=userdata.get(\"GLYN\"),\n",
        "    model=\"glm-4.5\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKy-BqvzPYXh"
      },
      "outputs": [],
      "source": [
        "def doc_parsing(file_path) -> list[Document]:\n",
        "    if file_path.endswith(\"pdf\"):\n",
        "        loader = PyPDFLoader(file_path=file_path)\n",
        "    elif file_path.endswith(\".txt\"):\n",
        "        loader = TextLoader(file_path=file_path)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        doc = loader.load()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading document: {e}\")\n",
        "        return []\n",
        "\n",
        "    semantic_splitter = SemanticChunker(\n",
        "        embeddings=embeddings,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=95\n",
        "    )\n",
        "    full_text = doc[0].page_content if doc else \"\"\n",
        "    if not full_text:\n",
        "        return []\n",
        "\n",
        "    raw_chunks = semantic_splitter.split_text(full_text)\n",
        "    print(f\"Number of chunks created: {len(raw_chunks)}\")\n",
        "    docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "    return docs\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XH_nld_PYU5"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def define_term(term: str) -> str:\n",
        "    \"\"\"Returns a one-sentence definition of a given AI term. Use this tool when a student asks 'What is X?' or 'define Y'.\"\"\"\n",
        "    definitions = {\n",
        "        \"transformer\": \"A model architecture that uses attention mechanisms to weigh the importance of different parts of the input sequence.\",\n",
        "        \"embedding\": \"A numerical representation of text that captures its semantic meaning for use in machine learning models.\",\n",
        "        \"rag\": \"A technique that retrieves information from an external knowledge base to improve the accuracy and relevance of an LLM's response.\",\n",
        "    }\n",
        "    return definitions.get(term.lower(), f\"I'm sorry, I don't have a definition for '{term}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bud-GFa5PYSJ"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def summarize_notes(text: str) -> str:\n",
        "    \"\"\"Takes a block of student notes (a paragraph) and returns a concise summary of 2-3 sentences. Use this tool when a student provides a long paragraph and asks to summarize it.\"\"\"\n",
        "    summary_chain = ChatPromptTemplate.from_template(\n",
        "        \"Summarize the following text in 2-3 sentences:\\n\\n{text}\"\n",
        "    ) | client\n",
        "    return summary_chain.invoke({\"text\": text}).content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo5ERrzRQgi4"
      },
      "outputs": [],
      "source": [
        "def retrieve_docs(question: str) -> str:\n",
        "    \"\"\"Retrieves relevant documents from the vector store based on a student's question.\"\"\"\n",
        "    if not vector_store.get_all_documents():\n",
        "        return \"No documents have been uploaded to search from.\"\n",
        "\n",
        "    SUB_QUERY_TEMPLATE = \"\"\"\n",
        "    You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "    Generate 3 diverse search queries related to the user's question, which can be used to retrieve relevant documents.\n",
        "    The queries should be concise and cover different aspects or angles of the original question.\n",
        "\n",
        "    Original Question: {question}\n",
        "\n",
        "    Generated Queries:\n",
        "    -\n",
        "    \"\"\"\n",
        "    sub_query_chain = ChatPromptTemplate.from_template(SUB_QUERY_TEMPLATE) | client\n",
        "    response = sub_query_chain.invoke({\"question\": question})\n",
        "    queries = [q.strip() for q in response.content.split('-') if q.strip()]\n",
        "\n",
        "    all_retrieved_docs = []\n",
        "    seen_doc_contents = set()\n",
        "    for query in queries:\n",
        "        retrieved_for_query = vector_store.similarity_search(query)\n",
        "        for doc in retrieved_for_query:\n",
        "            if doc.page_content not in seen_doc_contents:\n",
        "                all_retrieved_docs.append(doc)\n",
        "                seen_doc_contents.add(doc.page_content)\n",
        "\n",
        "    if not all_retrieved_docs:\n",
        "        return \"No relevant information found in the uploaded documents.\"\n",
        "\n",
        "    context_text = \"\\n\\n\".join(d.page_content for d in all_retrieved_docs)\n",
        "    return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqoikhIyQgfw"
      },
      "outputs": [],
      "source": [
        "rag_tool = tool(retrieve_docs)\n",
        "\n",
        "# List available tools for the agent\n",
        "tools = [define_term, summarize_notes, rag_tool]\n",
        "\n",
        "# Create the agent with its prompt\n",
        "template_with_tools = \"\"\"\n",
        "You are AICLASS assistant, a smart and helpful assistant at takenolab. You have access to specialized tools to help students with their course content.\n",
        "Your primary goal is to be supportive, precise, and direct.\n",
        "\n",
        "You must answer student questions by first determining if you need a tool.\n",
        "- If the question is about an AI term, use the `define_term` tool.\n",
        "- If the request is to summarize text, use the `summarize_notes` tool.\n",
        "- For all other questions about course content (e.g., \"What does the policy say about...?\", \"Explain topic X\"), use the `retrieve_course_content` tool.\n",
        "- If a question is not related to any of these tasks, kindly state that you cannot assist.\n",
        "\n",
        "student problem:\n",
        "{input}\n",
        "\n",
        "{agent_scratchpad}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template_with_tools)\n",
        "llm_with_tools = client.bind_tools(tools)\n",
        "agent = create_tool_calling_agent(llm_with_tools, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLgS_k0ZQgYQ"
      },
      "outputs": [],
      "source": [
        "def retrieve_and_answer_with_history(question: str, chat_history: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "    # Convert chat_history (list of dictionaries) to list of BaseMessage for Langchain\n",
        "    history_messages: List[BaseMessage] = []\n",
        "    for message in chat_history:\n",
        "        if message[\"role\"] == \"user\":\n",
        "            history_messages.append(HumanMessage(content=message[\"content\"]))\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            history_messages.append(AIMessage(content=message[\"content\"]))\n",
        "\n",
        "    try:\n",
        "        # Invoke the agent with the current question and formatted history\n",
        "        response = agent_executor.invoke({\"input\": question, \"chat_history\": history_messages})\n",
        "        bot_message = response.get(\"output\", \"I'm sorry, I couldn't generate a response.\")\n",
        "\n",
        "        # Append the new interaction to the history in the correct format\n",
        "        chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
        "\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred: {str(e)}\"\n",
        "        chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "        return chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4nuIeu6QgUo"
      },
      "outputs": [],
      "source": [
        "def doc_loader(file_path):\n",
        "    docs = doc_parsing(file_path)\n",
        "    if not docs:\n",
        "        return \"No content found or processed in the document.\"\n",
        "    _ = vector_store.add_documents(documents=docs)\n",
        "    return f\"Successfully added {len(docs)} document chunks.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Ct-OZSWJRI"
      },
      "outputs": [],
      "source": [
        "def interface():\n",
        "    iface = gr.ChatInterface(\n",
        "        fn=retrieve_and_answer_with_history,\n",
        "        chatbot=gr.Chatbot(height=200, type='messages', label=\"Assistant\"),\n",
        "        textbox=gr.Textbox(lines=2,submit_btn=True ),\n",
        "        title=\"Takenolab AIClass Assistant (Conversational RAG)\",\n",
        "        type='messages',\n",
        "        description=\"Ask a question about your course content and get smart advice, supporting multi-turn conversations.\",\n",
        "    )\n",
        "    docs_interface = gr.Interface(\n",
        "        fn=doc_loader,\n",
        "        inputs=gr.File(label=\"Choose a file to upload\",\n",
        "                       type='filepath',\n",
        "                       file_count='single',\n",
        "                       show_label=True\n",
        "                       ),\n",
        "        description=\"Upload a document to run retrieval‐augmented generation.\",\n",
        "        outputs=gr.TextArea()\n",
        "    )\n",
        "    table = gr.TabbedInterface(\n",
        "        [iface,docs_interface],\n",
        "        tab_names= ['Chat', \"Upload File for RAG\"],\n",
        "        title=\"LLM, RAG AND PROMPTS, Text Generation\"\n",
        "    )\n",
        "    iface.launch(debug=True, server_port=3000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "U9SksCbSYfIg",
        "outputId": "fadffa27-faf5-43f0-f05e-f0c31057819a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://974b0cc006747a2fc4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://974b0cc006747a2fc4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "Hello! I'm doing well, thank you for asking. I'm here to help you with your AI course content. If you have any questions about AI terms, need help summarizing notes, or want to explore course topics, feel free to ask!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "interface()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBp1LzwFloGI+tXg48VSNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}